"""
showcase.py â€” Highlight Reel Demo (v2)

showcase.py â€” Highlight Reel Demo
===================================
Runs PPO in a tuned environment designed to SHOW OFF:
  1. Smooth, safe cruising in the left lane
  2. Visible overtaking manoeuvres

Tricks used to make it look great on camera:
  - Fewer vehicles (20) â†’ more gaps â†’ agent can overtake instead of braking
  - initial_spacing = 1  â†’ vehicles start bunched â†’ more overtaking opportunities
  - scaling = 5.5        â†’ wider field of view â†’ you see more road
  - centering_position   â†’ ego car is left-centred so you see what's ahead
  - FPS 5 (default)      â†’ slow enough to follow every lane change
  - Prints action live   â†’ easy to add text overlay while recording

Usage:
    .venv\\Scripts\\python.exe scripts\\evaluation\\showcase.py
    .venv\\Scripts\\python.exe scripts\\evaluation\\showcase.py --fps 3 --episodes 5
    .venv\\Scripts\\python.exe scripts\\evaluation\\showcase.py --model ppo       (default)
    .venv\\Scripts\\python.exe scripts\\evaluation\\showcase.py --model rainbow
"""

import argparse
import random
import os
import sys
import time

import gymnasium as gym
import highway_env  # noqa: F401

from stable_baselines3 import DQN, PPO

# â”€â”€ Model registry â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODELS = {
    "ppo":         ("PPO",        "models/ppo/ppo_final",             PPO),
    "rainbow":     ("Rainbow DQN","models/rainbow_dqn/rainbow_final", DQN),
    "dueling":     ("Dueling DQN","models/dueling_dqn/dueling_final", DQN),
    "double":      ("Double DQN", "models/double_dqn/double_final",   DQN),
    "dqn":         ("DQN",        "models/dqn/dqn_final",             DQN),
}

ACTION_NAMES = {
    0: "â—€  LANE LEFT",
    1: "â”€â”€ IDLE",
    2: "LANE RIGHT â–¶",
    3: "â–²  FASTER",
    4: "â–¼  SLOWER",
}

ACTION_EMOJI = {
    0: "â¬…ï¸  overtaking!",
    1: "â¡ï¸  holding lane",
    2: "â¡ï¸  moving right",
    3: "â¬†ï¸  accelerating",
    4: "â¬‡ï¸  braking",
}

# â”€â”€ Showcase environment config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SHOWCASE_CONFIG = {
    "observation": {
        "type": "Kinematics",
        "vehicles_count": 15,
        "features": ["x", "y", "vx", "vy", "cos_h", "sin_h"],
        "normalize": True,
        "absolute": False,
    },
    "action": {"type": "DiscreteMetaAction"},
    "lanes_count": 4,
    "vehicles_count": 30,          # â† enough to create overtaking but not gridlock
    "duration": 40,
    "initial_spacing": 2,          # â† spaced out like DQN training â†’ no gridlock
    "collision_reward": -1,
    "right_lane_reward": 0.1,      # â† small right-lane nudge, no left-lane bias
    "high_speed_reward": 0.4,
    "reward_speed_range": [35, 40],  # â† agent targets 40 m/s
    "normalize_reward": True,
    "simulation_frequency": 5,
    "policy_frequency": 1,
    "speed_limit": 40,               # â† allow 40 m/s (default cap is 30)
    "other_vehicles_type": "highway_env.vehicle.behavior.IDMVehicle",
    "screen_width":  1400,
    "screen_height": 250,
    "centering_position": [0.25, 0.5],
    "scaling": 5.5,
}


# LeftLaneRewardWrapper removed â€” it was biasing the agent into lane 0 and causing crashes


# â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def bar(value, total, width=30):
    filled = int(round(width * value / total))
    return "â–ˆ" * filled + "â–‘" * (width - filled)


def print_live(ep, step, action, reward, lane, crashed):
    action_str = ACTION_EMOJI.get(action, "?")
    lane_str   = f"Lane {lane}"
    status     = "ğŸ’¥ CRASHED" if crashed else "âœ… Safe"
    print(
        f"\r  Ep {ep:2d} | Step {step:3d} | {action_str:<25} | "
        f"{lane_str} | R={reward:5.1f} | {status}   ",
        end="", flush=True
    )


# â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    parser = argparse.ArgumentParser(description="Highway RL Showcase")
    parser.add_argument("--model",    default="ppo",
                        choices=list(MODELS.keys()),
                        help="Which model to watch (default: ppo)")
    parser.add_argument("--episodes", type=int,   default=8,
                        help="Episodes to run (default: 8)")
    parser.add_argument("--fps",      type=float, default=5.0,
                        help="Render FPS â€” lower = slower (default: 5)")
    args = parser.parse_args()

    label, model_path, ModelClass = MODELS[args.model]
    frame_time = 1.0 / args.fps

    # Check model exists
    if not os.path.exists(model_path + ".zip"):
        # Try alternative path patterns
        alt = model_path.replace("_final", "_checkpoint_300000_steps")
        if not os.path.exists(alt + ".zip"):
            # Scan models folder for any matching file
            base = model_path.split("/")[1]  # e.g. "ppo"
            model_dir = f"models/{base}"
            if os.path.isdir(model_dir):
                zips = sorted([f for f in os.listdir(model_dir) if f.endswith(".zip")])
                if zips:
                    model_path = os.path.join(model_dir, zips[-1][:-4])
                    print(f"  Using latest checkpoint: {zips[-1]}")
                else:
                    print(f"âŒ  No model found in {model_dir}/")
                    sys.exit(1)
            else:
                print(f"âŒ  Model not found: {model_path}.zip")
                print(f"    Available model folders: {os.listdir('models')}")
                sys.exit(1)

    env = gym.make("highway-v0", render_mode="human", config=SHOWCASE_CONFIG)
    # No wrapper â€” agent drives with its original trained policy, no lane bias

    model = ModelClass.load(model_path, env=env)
    model.policy.set_training_mode(False)

    print()
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print(f"â•‘  ğŸš—  Highway RL Showcase â€” {label:<25} â•‘")
    print(f"â•‘  Episodes : {args.episodes:<5}  FPS : {args.fps:<6}  Model : {args.model:<10}  â•‘")
    print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
    print("â•‘  Watch for:                                          â•‘")
    print("â•‘  â¬…ï¸  Lane Left  = overtaking manoeuvre               â•‘")
    print("â•‘  â¬†ï¸  FASTER     = accelerating past slow traffic     â•‘")
    print("â•‘  âœ… reaching step 40 = full episode survived         â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()

    total_reward = 0.0
    total_safe   = 0
    overtakes    = 0

    for ep in range(1, args.episodes + 1):
        obs, _ = env.reset()

        # â”€â”€ Force slow traffic so the agent MUST overtake â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        try:
            ego = env.unwrapped.vehicle
            for v in env.unwrapped.road.vehicles:
                if v is not ego:
                    traffic_spd = random.uniform(20.0, 22.0)  # slower traffic â†’ safer overtakes
                    v.target_speed = traffic_spd
                    v.speed = min(v.speed, traffic_spd)
        except Exception:
            pass  # silently skip if vehicle API differs
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        done        = False
        ep_reward   = 0.0
        ep_steps    = 0
        collision   = False
        ep_overtakes = 0
        prev_lane   = None

        while not done:
            t0 = time.perf_counter()

            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            env.render()

            ep_reward += reward
            ep_steps  += 1
            done       = terminated or truncated
            action     = int(action)

            # Count overtakes (every LANE LEFT action)
            if action == 0:
                ep_overtakes += 1

            if info.get("crashed", False):
                collision = True

            # Current lane
            try:
                lane = env.unwrapped.vehicle.lane_index[2]
            except Exception:
                lane = "?"

            print_live(ep, ep_steps, action, ep_reward, lane, collision)

            elapsed = time.perf_counter() - t0
            sleep = frame_time - elapsed
            if sleep > 0:
                time.sleep(sleep)

        print()  # newline after live line

        status = "ğŸ’¥ CRASHED" if collision else "âœ… Safe"
        print(f"  â””â”€ Episode {ep:2d} done | Reward: {ep_reward:.1f} | "
              f"Steps: {ep_steps} | Overtakes: {ep_overtakes} | {status}")
        print()

        total_reward += ep_reward
        overtakes    += ep_overtakes
        if not collision:
            total_safe += 1

    env.close()

    print("â•" * 56)
    print(f"  ğŸ“Š Final Summary ({args.episodes} episodes)")
    print(f"  Mean reward   : {total_reward / args.episodes:.2f}")
    print(f"  Safe runs     : {total_safe} / {args.episodes}  "
          f"({100*total_safe//args.episodes}%)")
    print(f"  Total overtakes detected : {overtakes}")
    print("â•" * 56)


if __name__ == "__main__":
    main()
